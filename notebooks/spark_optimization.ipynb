{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyspark\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, broadcast\n",
                "import time\n",
                "\n",
                "# Initialize Spark Session with Optimized Configurations\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Spark Optimization\") \\\n",
                "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
                "    .config(\"spark.executor.memory\", \"8g\") \\\n",
                "    .config(\"spark.driver.memory\", \"4g\") \\\n",
                "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
                "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"\u2705 Spark Session Initialized with Optimized Configurations\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Sample Large Dataset\n",
                "df_large = spark.read.parquet(\"hdfs:///datasets/large_table.parquet\")\n",
                "df_small = spark.read.parquet(\"hdfs:///datasets/small_lookup.parquet\")\n",
                "\n",
                "# Broadcast Join Optimization\n",
                "start_time = time.time()\n",
                "df_joined = df_large.join(broadcast(df_small), \"user_id\")\n",
                "df_joined.count()\n",
                "print(f\"\u2705 Broadcast Join Execution Time: {time.time() - start_time:.2f} seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimize Partitioning Strategy\n",
                "df_repartitioned = df_large.repartition(100)  # Increase parallelism\n",
                "df_coalesced = df_large.coalesce(10)          # Reduce shuffle cost\n",
                "\n",
                "print(f\"\u2705 Repartitioned: {df_repartitioned.rdd.getNumPartitions()} partitions\")\n",
                "print(f\"\u2705 Coalesced: {df_coalesced.rdd.getNumPartitions()} partitions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Caching and Materializing Data\n",
                "df_large.cache()\n",
                "df_large.count()  # Trigger caching\n",
                "print(\"\u2705 Data Cached in Memory\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Monitoring Execution Plan\n",
                "df_large.explain(True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stop Spark Session\n",
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}